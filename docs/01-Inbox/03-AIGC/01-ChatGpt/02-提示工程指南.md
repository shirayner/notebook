---
sidebar_label: 提示工程指南
tags:
  - ChatGpt
  - AIGC
---
# 提示工程指南

## 推荐阅读

> - [What is Prompt Engineering?](https://www.promptengineering.org/what-is-prompt-engineering/)

其他：

- [ChatGPT是什么?](https://www.w3cschool.cn/openai_doc/openai_doc-5r4o3rob.html)
- [ChatGPT 提示的艺术：制作清晰有效咒语](https://github.com/wikieden/Awesome-ChatGPT-Prompts-CN/blob/main/ChatGpt-receipt.md)
- [【ChatGPT】一文教你怎么编写清晰有效的（Prompt）提示词~](https://juejin.cn/post/7215536461478707258)
- [【2023】猫娘方法持续讨论更新](https://github.com/PlexPt/awesome-chatgpt-prompts-zh/issues/12)
- [解除封印咒语](https://github.com/PlexPt/awesome-chatgpt-prompts-zh/blob/main/cat.md)
- [写好ChatGPT提示词的10个技巧](https://www.niaogebiji.com/article-553890-1.html)
- [在 ChatGPT 中构建虚拟机](https://www.w3cschool.cn/openai_doc/openai_doc-gr643rkm.html)

## 提示词参考

提示词网站：

> - [rockbenben/ChatGPT-Shortcut](https://github.com/rockbenben/ChatGPT-Shortcut)：Maximize your efficiency and productivity. 让生产力加倍的 ChatGPT 快捷指令，按照领域和功能分区，可对提示词进行标签筛选、关键词搜索和一键复制。

提示词开源项目：

> * [f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)

## 提示词教程

> * [datawhalechina/prompt-engineering-for-developers](https://github.com/datawhalechina/prompt-engineering-for-developers)
> * [AIGC - ChatGPT从入门到应用](https://datawhaler.feishu.cn/docx/MNHBdr7hqoEdNexMoi2cbYpcnTe)
> * [dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)

## 一、提示工程简介

提示工程（Prompt Engineering）是人工智能 (AI) 中的一个概念，特别是自然语言处理，主要关注于提示词的设计、改进和优化，从而指导大语言模型更好的完成特定任务。

### 1.大语言模型设置

使用提示词时，您会通过 API 或直接与大语言模型进行交互。你可以通过配置一些参数以获得不同的提示结果。

 **Temperature** ：简单来说，`temperature` 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。我们目前也在增加其他可能 token 的权重。在实际应用方面，对于质量保障（QA）等任务，我们可以设置更低的 `temperature` 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，你可以适当调高 `temperature` 参数值。

 **Top_p** ：同样，使用 `top_p`（与 `temperature` 一起称为核采样的技术），可以用来控制模型返回结果的真实性。如果你需要准确和事实的答案，就把参数值调低。如果你想要更多样化的答案，就把参数值调高一些。

一般建议是改变其中一个参数就行，不用两个都调整。

在我们开始一些基础示例之前，请记住最终生成的结果可能会和使用的大语言模型的版本不同有关。

### 2.提示词格式

标准提示词应该遵循以下格式：

```
<问题>?
```

或

```
<指令>
```

这种可以被格式化为标准的问答格式，如：

```
Q: <问题>?A: 
```

以上的提示方式，也被称为**零样本提示（zero-shot prompting）**，即用户不提供任务结果相关的示范，直接提示语言模型给出任务相关的回答。某些大型语言模式有能力实现零样本提示，但这也取决于任务的复杂度和已有的知识范围。

基于以上标准范式，目前业界普遍使用的还是更高效的_小样本提示（Few-shot Prompting）_范式，即用户提供少量的提示范例，如任务说明等。小样本提示一般遵循以下格式：

```
<问题>?
<答案>
<问题>?
<答案>
<问题>?
<答案>
<问题>?
```

而问答模式即如下：

```
Q: <问题>?
A: <答案>
Q: <问题>?
A: <答案>
Q: <问题>?
A: <答案>
Q: <问题>?
A:
```

注意，使用问答模式并不是必须的。你可以根据任务需求调整提示范式。比如，您可以按以下示例执行一个简单的分类任务，并对任务做简单说明：

*提示词*

```
This is awesome! // Positive
This is bad! // Negative
Wow that movie was rad! // Positive
What a horrible show! //
```

*输出结果*

```
Negative
```

语言模型可以基于一些说明了解和学习某些任务，而小样本提示正好可以赋能上下文学习能力。

### 3.提示词要素

提示词可以包含以下任意要素：

- **指令** ：想要模型执行的特定任务或指令。
- **上下文** ：包含外部信息或额外的上下文信息，引导语言模型更好地响应。
- **输入数据** ：用户输入的内容或问题。
- **输出指示** ：指定输出的类型或格式。

注意，提示词所需的格式取决于您想要语言模型完成的任务类型，并非所有以上要素都是必须的。


## 二、提示词常见场景

文本总结、信息提取、文本分类、推理


## 三、提示词技术

### 1.零样本提示

如今，经过大量数据训练并调整指令的LLM能够执行零样本任务。我们在前一节中尝试了一些零样本示例。以下是我们使用的一个示例：

*提示：*

```
将文本分类为中性、负面或正面。
文本：我认为这次假期还可以。
情感：
```

*输出：*

```
中性
```

请注意，在上面的提示中，我们没有向模型提供任何示例——这就是零样本能力的作用。

指令调整已被证明可以改善零样本学习[Wei等人（2022）(opens in a new tab)](https://arxiv.org/pdf/2109.01652.pdf)。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，[RLHF(opens in a new tab)](https://arxiv.org/abs/1706.03741)（来自人类反馈的强化学习）已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。这一最新发展推动了像ChatGPT这样的模型。我们将在接下来的章节中讨论所有这些方法和方法。

当零样本不起作用时，建议在提示中提供演示或示例，这就引出了少样本提示。


### 2.少样本提示

#### 少样本提示的示例

虽然大型语言模型展示了惊人的零样本能力，但在使用**零样本**设置时，它们**在更复杂的任务上仍然表现不佳**。少样本提示可以作为一种技术，以启用**上下文学习**，我们在提示中提供演示以引导模型实现更好的性能。演示作为后续示例的条件，我们希望模型生成响应。

让我们通过[Brown等人2020年(opens in a new tab)](https://arxiv.org/abs/2005.14165)提出的一个例子来演示少样本提示。在这个例子中，任务是在句子中正确使用一个新词。

*提示：*

```
“whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是：
我们在非洲旅行时看到了这些非常可爱的whatpus。
“farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是：
```

*输出：*

```
当我们赢得比赛时，我们都开始庆祝跳跃。
```

我们可以观察到，模型通过提供一个示例（即1-shot）已经学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示（例如3-shot、5-shot、10-shot等）。

根据[Min等人（2022）(opens in a new tab)](https://arxiv.org/abs/2202.12837)的研究结果，以下是在进行少样本学习时关于演示/范例的一些额外提示：

* “标签空间和演示指定的输入文本的分布都很重要（无论标签是否对单个输入正确）”
* 使用的格式也对性能起着关键作用，即使只是使用随机标签，这也比没有标签好得多。
* 其他结果表明，从真实标签分布（而不是均匀分布）中选择随机标签也有帮助。

让我们尝试一些例子。让我们首先尝试一个随机标签的例子（意味着将标签Negative和Positive随机分配给输入）：

*提示：*

```
这太棒了！// Negative这太糟糕了！// Positive哇，那部电影太棒了！// Positive多么可怕的节目！//
```

*输出：*

```
Negative
```

即使标签已经随机化，我们仍然得到了正确的答案。请注意，我们还保留了格式，这也有助于。实际上，通过进一步的实验，我们发现我们正在尝试的新GPT模型甚至对随机格式也变得更加稳健。例如：

*提示：*

```
Positive This is awesome! This is bad! NegativeWow that movie was rad!PositiveWhat a horrible show! --
```

*输出：*

```
Negative
```

上面的格式不一致，但模型仍然预测了正确的标签。我们必须进行更彻底的分析，以确认这是否适用于不同和更复杂的任务，包括提示的不同变体。

#### 少样本提示的限制

标准的少样本提示对许多任务都有效，但仍然不是一种完美的技术，特别是在处理更复杂的推理任务时。让我们演示为什么会这样。您是否还记得之前提供的任务：

```
这组数字中的奇数加起来是一个偶数：15、32、5、13、82、7、1。A：
```

如果我们再试一次，模型输出如下：

```
是的，这组数字中的奇数加起来是107，是一个偶数。
```

这不是正确的答案，这不仅突显了这些系统的局限性，而且需要更高级的提示工程。

让我们尝试添加一些示例，看看少样本提示是否可以改善结果。

*提示：*

```
这组数字中的奇数加起来是一个偶数：4、8、9、15、12、2、1。
A：答案是False。
这组数字中的奇数加起来是一个偶数：17、10、19、4、8、12、24。
A：答案是True。
这组数字中的奇数加起来是一个偶数：16、11、14、4、8、13、24。
A：答案是True。
这组数字中的奇数加起来是一个偶数：17、9、10、12、13、4、2。
A：答案是False。
这组数字中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A：
```

*输出：*

```
答案是True。
```

这没用。似乎少样本提示不足以获得这种类型的推理问题的可靠响应。上面的示例提供了任务的基本信息。如果您仔细观察，我们引入的任务类型涉及几个更多的推理步骤。换句话说，如果我们将问题分解成步骤并向模型演示，这可能会有所帮助。最近，[思维链（CoT）提示(opens in a new tab)](https://arxiv.org/abs/2201.11903) 已经流行起来，以解决更复杂的算术、常识和符号推理任务。

总的来说，提供示例对解决某些任务很有用。当零样本提示和少样本提示不足时，这可能意味着模型学到的东西不足以在任务上表现良好。从这里开始，建议开始考虑微调您的模型或尝试更高级的提示技术。接下来，我们将讨论一种流行的提示技术，称为思维链提示，它已经获得了很多关注。


### 3.思维链提示

#### 链式思考（CoT）提示

![COT](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)

图片来源：[Wei等人（2022）(opens in a new tab)](https://arxiv.org/abs/2201.11903)

在[Wei等人（2022）(opens in a new tab)](https://arxiv.org/abs/2201.11903)中引入的链式思考（CoT）提示通过中间推理步骤实现了复杂的推理能力。您可以将其与少样本提示相结合，以获得更好的结果，以便在回答之前进行推理的更复杂的任务。

*提示：*

```
这组数中的奇数加起来是偶数：4、8、9、15、12、2、1。
A：将所有奇数相加（9、15、1）得到25。答案为False。
这组数中的奇数加起来是偶数：17、10、19、4、8、12、24。
A：将所有奇数相加（17、19）得到36。答案为True。
这组数中的奇数加起来是偶数：16、11、14、4、8、13、24。
A：将所有奇数相加（11、13）得到24。答案为True。
这组数中的奇数加起来是偶数：17、9、10、12、13、4、2。
A：将所有奇数相加（17、9、13）得到39。答案为False。
这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。
A：
```

*输出：*

```
将所有奇数相加（15、5、13、7、1）得到41。答案为False。
```

哇！我们可以看到在提供推理步骤时得到了完美的结果。实际上，我们可以通过提供更少的示例来解决此任务，即仅一个示例似乎就足够了：

*提示：*

```
这组数中的奇数加起来是偶数：4、8、9、15、12、2、1。A：将所有奇数相加（9、15、1）得到25。答案为False。这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。A：
```

*输出：*

```
将所有奇数相加（15、5、13、7、1）得到41。答案为False。
```

请记住，作者声称这是足够大的语言模型才会出现的新兴能力。


#### 零样本COT提示

![Zero-shot COT](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)

图片来源：[Kojima等人（2022）(opens in a new tab)](https://arxiv.org/abs/2205.11916)

最近提出的一个新想法是[零样本CoT(opens in a new tab)](https://arxiv.org/abs/2205.11916)（Kojima等人，2022年），它基本上涉及将“让我们逐步思考”添加到原始提示中。让我们尝试一个简单的问题，看看模型的表现如何：

*提示：*

```
我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？
```

*输出：*

```
11个苹果
```

答案是不正确的！现在让我们尝试使用特殊提示。

*提示：*

```
我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？
让我们逐步思考。
```

*输出：*

```
首先，您从10个苹果开始。
您给了邻居和修理工各2个苹果，所以您还剩下6个苹果。
然后您买了5个苹果，所以现在您有11个苹果。
最后，您吃了1个苹果，所以您还剩下10个苹果。
```

令人印象深刻的是，这个简单的提示在这个任务中非常有效。这在您没有太多示例可用于提示时特别有用。


### 4.自我一致性

也许在提示工程中更高级的技术之一是自我一致性。由[Wang等人（2022）(opens in a new tab)](https://arxiv.org/pdf/2203.11171.pdf)提出，自我一致性旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本CoT采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高CoT提示在涉及算术和常识推理的任务中的性能。

让我们尝试以下算术推理示例：

*提示：*

```
当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
```

*输出：*

```
35
```

输出是错误的！我们如何通过自我一致性来改进这个问题？让我们试试。我们将使用Wang等人2022年的少量样本范例（表17）：

*提示：*

```
Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？
A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 = 6棵树。答案是6。
Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？
A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 = 5辆汽车。答案是5。
Q：Leah有32块巧克力，她的姐姐有42块。如果他们吃了35块，他们总共还剩多少块？
A：Leah有32块巧克力，Leah的姐姐有42块。这意味着最初有32 + 42 = 74块巧克力。已经吃了35块。因此，他们总共还剩74-35 = 39块巧克力。答案是39。
Q：Jason有20个棒棒糖。他给Denny一些棒棒糖。现在Jason只有12个棒棒糖。Jason给Denny多少棒棒糖？
A：Jason有20个棒棒糖。因为他现在只有12个，所以他必须把剩下的给Denny。他给Denny的棒棒糖数量必须是20-12 = 8个棒棒糖。答案是8。
Q：Shawn有五个玩具。圣诞节，他从他的父母那里得到了两个玩具。他现在有多少个玩具？
A：他有5个玩具。他从妈妈那里得到了2个，所以在那之后他有5 + 2 = 7个玩具。然后他从爸爸那里得到了2个，所以总共他有7 + 2 = 9个玩具。答案是9。
Q：服务器房间里有9台计算机。从周一到周四，每天都会安装5台计算机。现在服务器房间里有多少台计算机？
A：从周一到周四有4天。每天都添加了5台计算机。这意味着总共添加了4 * 5 =
20台计算机。一开始有9台计算机，所以现在有9 + 20 = 29台计算机。答案是29。
Q：Michael有58个高尔夫球。星期二，他丢失了23个高尔夫球。星期三，他又丢失了2个。星期三结束时他还剩多少个高尔夫球？
A：Michael最初有58个球。星期二他丢失了23个，所以在那之后他有58-23 = 35个球。星期三他又丢失了2个，所以现在他有35-2 = 33个球。答案是33。
Q：Olivia有23美元。她用每个3美元的价格买了五个百吉饼。她还剩多少钱？
A：她用每个3美元的价格买了5个百吉饼。这意味着她花了5
Q：当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
A：

```

*输出1：*

```
当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70-3 = 67岁。答案是67。
```

*输出2：*

```
当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。现在叙述者70岁了，他的妹妹应该是70-3 = 67岁。答案是67。
```

*输出3：*

```
当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70/2 = 35岁。答案是35。
```

计算最终答案涉及几个步骤（详见论文），但为了简单起见，我们可以看到已经出现了大多数答案，因此这基本上将成为最终答案。

### 5.生成知识提示

![GENKNOW](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fgen-knowledge.055b8d37.png&w=1920&q=75)

图片来源：[Liu等人2022(opens in a new tab)](https://arxiv.org/pdf/2110.08387.pdf)

LLM继续得到改进，其中一种流行的技术是能够融合知识或信息，以帮助模型做出更准确的预测。

使用类似的思路，模型是否也可以在做出预测之前用于生成知识呢？这就是[Liu等人2022(opens in a new tab)](https://arxiv.org/pdf/2110.08387.pdf)的论文所尝试的——生成知识以作为提示的一部分。特别是，这对于常识推理等任务有多大帮助？

让我们尝试一个简单的提示：

*提示：*

```
高尔夫球的一部分是试图获得比其他人更高的得分。是或否？
```

*输出：*

```
是。
```

这种错误揭示了LLM在执行需要更多关于世界的知识的任务时的局限性。我们如何通过生成知识来改进呢？

首先，我们生成一些“知识”：

*提示：*

```
输入：希腊比墨西哥大。
知识：希腊的面积约为131,957平方公里，而墨西哥的面积约为1,964,375平方公里，使墨西哥比希腊大了1,389%。
输入：眼镜总是会起雾。
知识：当你的汗水、呼吸和周围的湿度中的水蒸气落在冷的表面上，冷却并变成微小的液滴时，会在眼镜镜片上产生冷凝。你看到的是一层薄膜。你的镜片相对于你的呼吸会比较凉，尤其是当外面的空气很冷时。
输入：鱼有思考能力。
知识：鱼比它们看起来更聪明。在许多领域，如记忆力，它们的认知能力与或超过非人类灵长类动物等“更高级”的脊椎动物。鱼的长期记忆帮助它们跟踪复杂的社交关系。
输入：一个人一生中吸烟很多香烟的常见影响是患肺癌的几率高于正常水平。
知识：那些一生中平均每天吸烟不到一支香烟的人，患肺癌的风险是从不吸烟者的9倍。在每天吸烟1到10支香烟之间的人群中，死于肺癌的风险几乎是从不吸烟者的12倍。
输入：一块石头和一颗卵石大小相同。
知识：卵石是一种根据Udden-Wentworth沉积学尺度的颗粒大小为4到64毫米的岩屑。卵石通常被认为比颗粒（直径2到4毫米）大，比卵石（直径64到256毫米）小。
输入：高尔夫球的一部分是试图获得比其他人更高的得分。
知识：
```

*知识1：*

```
高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。
```

*知识2：*

```
高尔夫球是一项精密的球杆运动，竞争选手（或高尔夫球手）使用许多类型的球杆，用最少的杆数将球打入球场上一系列洞中。目标是以最低的分数完成比赛，该分数是通过将每个洞上所用的杆数相加计算出来的。得分最低的选手赢得比赛。
```

我们使用[Liu等人2022(opens in a new tab)](https://arxiv.org/pdf/2110.08387.pdf)的论文中提供的提示。

下一步是将知识整合并得出预测。我将问题重新格式化为QA格式，以指导答案格式。

*提示：*

```
问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？
知识：高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。
解释和答案： 
```

*答案1（置信度非常高）：*

```
不是，高尔夫球的目标不是获得比其他人更高的得分。相反，目标是以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。
```

```
问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？
知识：高尔夫球是一项精密的球杆运动，竞争选手（或高尔夫球手）使用许多类型的球杆，用最少的杆数将球打入球场上一系列洞中。目标是以最低的分数完成比赛，该分数是通过将每个洞上所用的杆数相加计算出来的。得分最低的选手赢得比赛。
解释和答案：
```

*答案2（置信度要低得多）：*

```
是的，高尔夫球的一部分是试图获得比其他人更高的得分。每个选手都试图以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。得分最低的选手赢得比赛。
```

这个例子发生了一些非常有趣的事情。在第一个答案中，模型非常自信，但在第二个答案中不太自信。我简化了过程以进行演示，但在得出最终答案时还有一些细节需要考虑。请查看论文以了解更多。


## 二、提示原则

### 1.清晰明确的指令

#### 1.1 使用分隔符

> - 你可以使用任何明显的标点符号将特定的文本部分与提示的其余部分分开。这可以是任何可以使模型明确知道这是一个单独部分的标记。
> - 使用分隔符是一种可以避免提示注入的有用技术。

#### 1.2 结构化的输出

> * 要求生成一个结构化的输出，这可以使模型的输出更可读，或更容易被我们解析。
> * Json、表格等

#### 1.3 **要求模型检查是否满足条件**

如果任务做出的假设不一定满足，我们可以告诉模型先检查这些假设，如果不满足，指示并停止执行。你还可以考虑潜在的边缘情况以及模型应该如何处理它们，以避免意外的错误或结果。

#### 1.4 提供少量示例

即在要求模型执行实际任务之前，提供给它少量成功执行任务的示例。

### 2.给模型时间去思考

如果模型匆忙地得出了错误的结论，您应该尝试重新构思查询，请求模型在提供最终答案之前进行一系列相关的推理。换句话说，如果您给模型一个在短时间或用少量文字无法完成的任务，它可能会猜测错误。

#### 2.1 **指定完成任务所需的步骤**

将复杂任务拆分成多个多个步骤

#### **2.2 指导模型在下结论之前找出一个自己的解法**

## 二、如何写好提示词

提示词的基本原则

提示词基本公式：时间、地点、人物、背景、目标、任务
